{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvKC5YPwrdjV15ScQlL8sy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mel-iza/The-Natural-Language-Processing-Workshop/blob/main/NLP_Workshop_Chapter_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> <b>The Natural Language Processing Workshop</b> </h1>\n",
        "Rohan Chopra, Aniruddha M. Godbole, Nipun Sadvilkar et al, 2020. Packt Publishing."
      ],
      "metadata": {
        "id": "MxKRwHSF8Hd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chapter 1: Introduction to Natural Language Processing"
      ],
      "metadata": {
        "id": "flB2iDrKY5JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview: In this chapter, you will learn the difference between **Natural Language Processing (NLP)** and basic text analysis. You will implement various preprocessing tasks such as tokenization, lemmatization, stemming, stop word removal, and more. By the end of this chapter, you will have a deep understanding of the various phases of an NLP project, from data collection to model deployment."
      ],
      "metadata": {
        "id": "ZgEjGHhgDWxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **What natural language is?** <br>\n",
        "It is a means for us to express our thoughts and ideas. To define it more specifically, language is a mutually agreed upon set of protocols involving words/sounds that we use to communicate with each other.\n",
        "\n",
        "NLP can be defined as a field of computer science that is concerned with enabling compouter algorithms to understand, analyze, and generate natural languages.\n"
      ],
      "metadata": {
        "id": "UscKTUcPD8wg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP works at different leves, with means that machines process and understand natural language at different levels:\n",
        "- **Morphological level**: this leval deals with understanding word structure and word information;\n",
        "- **Lexical level**: This level deals with understanding the part od speech of the word. <i>(compreensão da parte gramatical da palavra)</i>\n",
        "- **Syntatic level**: This level deals with understanding the syntatic analysis of a sentence, or parsing a sentence.\n",
        "- **Semantic level**: This deals with understanding the actual meaning of a sentence.\n",
        "- **Discourse level**: This level deals with understanding the meaning of a sentence beyound just the sentence level, that is, considering the context.\n",
        "- **Pragmatic level**: This level deals with using real-world knowledge to understand sentence. "
      ],
      "metadata": {
        "id": "VCkoYDF1Fi9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**History of NLP** <br>\n",
        "\n",
        "**NLP** = Artificial intelligence + linguistics + data science \n",
        "\n",
        "With the advancemnt of computing technologies and increased availability of data, NLP has undergone a huge change. Previously a traditional rule-based system was used for computations, in wich you had to explicitly write hardcoded rules. Today, compuations on natural language are being done using machine learning and deep learning techniques."
      ],
      "metadata": {
        "id": "LOAuEmh7avne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<i>O exemplo utilizado para ilustrar como os sistemas de NLP eram utilizados de acordo com regras, foi baseado em um projeto de extrair nomes de políticos de um jornal. Se quisermos pegar esses nomes, antes teríamos que elaborar todas as regras para eles, como por exemplo, qual seria a estrutura sintática de um nome próprio - um nome próprio precisaria começar sempre com letra maiúscula -  e assim por diante.\n",
        "\n",
        "Muito embora esse sistema baseado em regras não trouxesse um desempenho computacional, foi utilizado por bastante tempo.</i>"
      ],
      "metadata": {
        "id": "zcjY1tbCIurm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Text Analytics and NLP </b>"
      ],
      "metadata": {
        "id": "O0I7OpYY81me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Text analytics</b>  is the method of extracting meaningful insights and answering questions from text data ➡ (<i>length of sentences, length os words, word count, and finding words from the text</i>)\n",
        "\n",
        "↪ we are generating insights from text without getting into semantics of the language.\n",
        "\n",
        "<b>NLP</b> on the oter hand, help us in understanding the semantics and the underlying meaning of text  ➡ (<i> sentiment of a sentence, top keywords in a text, parts pf speech for different words</i>)\n",
        " \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zVCNJtPrYGKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* <b>Natural Language Understading (NLU) </b><br>\n",
        "NLU refres to a process by wich an inanimate object with computing power is able to comprehend spoken language.\n",
        "\n",
        "* <b>Natural Language Generatuion (NLG) </b><br>\n",
        "NLG refers to a process by wich an inanimate object with computing power is able to comunicate with humans in a language that they can understand or is able to generate human-understandable text from a dataset.\n"
      ],
      "metadata": {
        "id": "FrrJS7Z4gpWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1.01: Basic Text Analytics"
      ],
      "metadata": {
        "id": "jodYWNNb9WMp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ6eM4-CUf7n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1939908d-dda2-4af3-f612-e0579553c26b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The quick brown fox jumps over the lazy dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# 2. Assign a sentence variable the value 'the quick brown fox jumps over te lazy dog'\n",
        "\n",
        "sentence = 'The quick brown fox jumps over the lazy dog'\n",
        "sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Check if the word 'quick' belongs to sentence\n",
        "\n",
        "def find_word(word, sentence):\n",
        "  return word in sentence\n",
        "\n",
        "find_word('quick', sentence)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BLzybEgRz4G",
        "outputId": "2f466b39-058d-4275-cf0b-edf885b9557b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Find out the index value of the word 'fox'\n",
        "\n",
        "def get_index(word, text):\n",
        "  return text.index(word)\n",
        "\n",
        "get_index('fox', sentence)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPR2kCLuSGyv",
        "outputId": "215edf90-ca64-4017-bbdb-9ec25d3d0904"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Find out the rank of the word 'lazy'\n",
        "\n",
        "get_index('lazy', sentence.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwtx9he3T0de",
        "outputId": "1bb52dec-2a03-497c-fe96-895e2ecdf4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Print the third word of the given text\n",
        "\n",
        "def get_word(text, rank):\n",
        "  return text.split()[rank]\n",
        "\n",
        "get_word(sentence, 2)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "X9fY1JQmUBCS",
        "outputId": "c0ab37ac-ee29-4752-a83b-5383e2f67685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'brown'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Print the third word of the given text in reverse order\n",
        "\n",
        "get_word(sentence, 2)[::-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ROVvYXR1Ub3e",
        "outputId": "8c7d1f83-7763-47f0-9f47-0bb543e1774a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nworb'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Concatenate the first and last word ofs of the given sentence\n",
        "\n",
        "def concat_words(text):\n",
        "  '''\n",
        "  This method will concat first and last words of given text\n",
        "  '''\n",
        "  words = text.split()\n",
        "  first_word = words[0]\n",
        "  last_word = words[len(words)-1]\n",
        "  return first_word + last_word\n",
        "\n",
        "concat_words(sentence)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EynXKvqLVLun",
        "outputId": "88e7b7de-49d9-4dca-9a90-712fbe25066f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Thedog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Print words at even positions\n",
        "\n",
        "def get_even_positions_words(text):\n",
        "  words = text.split()\n",
        "  return [words[i] for i in range (len(words)) if i%2 == 0]\n",
        "\n",
        "get_even_positions_words(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TB6BUfBWFZL",
        "outputId": "d60f2527-8629-4d5a-9719-5247cfd4742f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'brown', 'jumps', 'the', 'dog']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Print the last  three letters of the text\n",
        "\n",
        "def get_last_n_letters(text, n):\n",
        "  return text[-n:]\n",
        "\n",
        "get_last_n_letters(sentence, 3)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x7Ads8e7WpEe",
        "outputId": "c90728e3-4f98-4bdf-b093-280a244ead7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. To print the text in reverse order\n",
        "\n",
        "def get_reverse(text):\n",
        "  return text[::-1]\n",
        "\n",
        "get_reverse(sentence)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F4ZyyDksW7Hf",
        "outputId": "eedf628d-56eb-481e-86d1-8c7cf3474752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'god yzal eht revo spmuj xof nworb kciuq ehT'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. To print each word of the given text in reverse order, maintaining their sequence\n",
        "\n",
        "def get_word_reverse(text):\n",
        "  words = text.split()\n",
        "  return ' '.join([word[::-1] for word in words])\n",
        "\n",
        "get_word_reverse(sentence)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kBB-UEGrXWtY",
        "outputId": "e01d8e80-c1f0-4ad4-8e6d-4346af038149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ehT kciuq nworb xof spmuj revo eht yzal god'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Various Tasks in NLP</b>"
      ],
      "metadata": {
        "id": "jrSwt9HS9bw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT_RijnnYhKp",
        "outputId": "3f02269d-176d-43d3-a3f0-4f859eeabfcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Tokenization</b> <br>\n",
        "<i>Tokenization refres to the procedure of splitting a sentence into its constituent parts - the words and punctuation that is it made up of. Such tokens are called <b>unigrams.</b></i>\n",
        "\n",
        "<br>\n",
        "Exercise 1.02: Tokenization of a simple sentence"
      ],
      "metadata": {
        "id": "2NzAo9l49iUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, download \n",
        "\n",
        "download(['punkt', 'averaged_perceptron_tagger', 'stopwords'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vd4WLzyG6KN5",
        "outputId": "87886c83-b2f7-4d32-bd98-0abe02414173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens(sentence):\n",
        "  words = word_tokenize(sentence)\n",
        "  return words"
      ],
      "metadata": {
        "id": "I91e_yBMakPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_tokens('I am reading NLP Fundamentals.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em1vxRWPa7vX",
        "outputId": "fd59fe38-48a0-4ebf-fe5b-ad27fa837dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'reading', 'NLP', 'Fundamentals', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>PoS Tagging (Parts-of-Speech)</b><br>\n",
        "<i>PoS Tagging refers to the process of tagging words within sentences with their respective PoS. We extract the PoS of tokens constituting a sentence so that we can filter out the PoS that are of interest and analyze them.</i>\n",
        "\n",
        "```\n",
        "DT = Determiner\n",
        "NN = Noun, common, singular or mass\n",
        "VBZ = Verb, present tense, third-person singular\n",
        "JJ = Adjective\n",
        "```\n",
        "\n",
        "PoS tagging finds application in many NLP tasks, including word sense disambiguation, classification, Named Entity recognition(NER) and coreference resolution.\n",
        "\n",
        "<br>\n",
        "Exercise 1.03: PoS Tagging\n"
      ],
      "metadata": {
        "id": "g7fFXlW7904k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "def get_tokens(sentence):\n",
        "  words = word_tokenize(sentence)\n",
        "  return words"
      ],
      "metadata": {
        "id": "NgCom0vv9-6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = get_tokens('I am reading NLP Fundamentals')\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAb6ofeKdV__",
        "outputId": "dd714045-8d9a-4580-88ae-d3bc61242bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'reading', 'NLP', 'Fundamentals']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos(words):\n",
        "  return pos_tag(words)\n",
        "\n",
        "get_pos(words)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqyy3wq5dqMD",
        "outputId": "3137fed5-841b-42fc-8170-ddd5f865f230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('reading', 'VBG'),\n",
              " ('NLP', 'NNP'),\n",
              " ('Fundamentals', 'NNS')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Stop Word Removal </b><br>\n",
        "<i>Stop words are the most frequently occuring words in any language and they are just used to support the construction of sentences and do not contribute anything to the semantics of a sentence. So, we can remove stop words from any text before an NLP process, as they occur very frequently and their presence doesn't have much impact on the sense of a sentence.</i>\n",
        "\n",
        "\n",
        "<br>\n",
        "Exercise 1.04: Stop Word Removal"
      ],
      "metadata": {
        "id": "E645LpRc-A77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download \n",
        "download('stopwords')\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gha13Hqz-Go7",
        "outputId": "d44ed871-df8a-4fc9-f4a4-f3eaf23371f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tem em portugues também!\n",
        "\n",
        "stop_words = stopwords.words('portuguese')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbUoQS8wfrgx",
        "outputId": "6803147f-9d88-40bb-d5b4-31ef30792faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'às', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'éramos', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estou', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'formos', 'fosse', 'fossem', 'fôssemos', 'fui', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'não', 'nas', 'nem', 'no', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'são', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'será', 'serão', 'serei', 'seremos', 'seria', 'seriam', 'seríamos', 'seu', 'seus', 'só', 'somos', 'sou', 'sua', 'suas', 'também', 'te', 'tem', 'tém', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terá', 'terão', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tínhamos', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tu', 'tua', 'tuas', 'um', 'uma', 'você', 'vocês', 'vos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM6QLFOAfz6v",
        "outputId": "9a8867cf-100a-4f3c-d224-3b4a39eb114e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove the stop words from a sentence, we first assign a string to the sentence variable and tokenize it into words using <b>word_tokenize()</b> method."
      ],
      "metadata": {
        "id": "3y0u_2Kri8Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I am learning Python. It is one of the '\\\n",
        "            'most popular programming languages'\n",
        "\n",
        "sentence_words = word_tokenize(sentence)            "
      ],
      "metadata": {
        "id": "VGsqHpgyi3FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvOy8X6gjf0Y",
        "outputId": "0e9f194d-ab82-42f7-d2ad-269f75ca2795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'languages']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To remove the stop words, we need to loop through each word in the sentence, check whether there are any stop words, and then finally combine them to form a complete sentence."
      ],
      "metadata": {
        "id": "nnl-L-G9jyK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(sentence_words, stop_words):\n",
        "  return ' '.join([word for word in sentence_words if \\\n",
        "                   word not in stop_words])"
      ],
      "metadata": {
        "id": "MU-qyeVckEl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(remove_stop_words(sentence_words, stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1Pm6dDEmHw7",
        "outputId": "3c25123a-c8fe-42f6-ac48-0c1beb793c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I learning Python . It one popular programming languages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your own stop words to the stop word list\n",
        "\n",
        "stop_words.extend(['I', 'It', 'one'])\n",
        "print(remove_stop_words(sentence_words, stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3HW3s26mib-",
        "outputId": "5d50d5e5-234f-4a2a-c2a7-e1f9ec4f48a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning Python . popular programming languages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Text Normalization </b><br>\n",
        "<i>There are some words that are spelled, pronounce, and represented differently. Although they are different, they refer to the same thing. Text normalization is a process wherein different variations of text get converted into a standard form.</i>\n",
        "\n",
        "In this exercise, we will normalize some given text. Basically, we will be trying to replace select words with new words, using the <b>replace()</b> function and finally produce the normalized text.<b>replace()</b>\n",
        "\n",
        "\n",
        "<br>\n",
        "Exercise 1.05: Text Normalization"
      ],
      "metadata": {
        "id": "Xng583jq-IPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'I visited te US from the UK on 22-20-18'"
      ],
      "metadata": {
        "id": "ac1sOwhh-O6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to replace `US` with `United States`, `UK` with `United Kingdom` and `18` with `2018`."
      ],
      "metadata": {
        "id": "W4RC2n4JowcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "  return text.replace('US', 'United States')\\\n",
        "             .replace('UK', 'United Kingdom')\\\n",
        "             .replace('-18', '-2018')"
      ],
      "metadata": {
        "id": "ztlSzxqEpCj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_sentence = normalize(sentence)\n",
        "print(normalized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQyqgv-GpZWI",
        "outputId": "f69d00df-c8fc-4746-b9e3-31246a52a854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I visited te United States from the United Kingdom on 22-20-2018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_sentence = normalize('US and UK are two superpowers')\n",
        "print(normalized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kasnguzapmm-",
        "outputId": "20f7b0ee-aa1b-4fe0-b699-72ab434bb99d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States and United Kingdom are two superpowers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Spelling Correction </b><br>\n",
        "<i>Spelling Correction is one of the most important tasks in NLP project. Ic can be time-consuming, but without it, there are high chances of losing out important information.</i>\n",
        "\n",
        "Spelling Correction is executed in two steps:\n",
        "1. Identify the misspelled word\n",
        "2. Replace it or suggest the correctly spelled word. There are a lot of algorithms for this task - one of them is the minimum edit distance, wich chooses the nearest correctly spelled word for a misspelled word.\n",
        "\n",
        "\n",
        "We make use of the <b>autocorrect</b> Python library to correct spellings.\n",
        "\n",
        "<br>\n",
        "Exercise 1.06: Spelling Correction of a word and a Sentence"
      ],
      "metadata": {
        "id": "Mhil3P1a_Pcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autocorrect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5H16ATsrQ-c",
        "outputId": "39e293d6-b9ec-4c48-c568-3500e4493edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 KB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622381 sha256=ade8de6270cf7f9646b7cefb33ac99483910e309da34cc583d3b5768a872186b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/0f/23/3c010c3fd877b962146e7765f9e9b08026cac8b035094c5750\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from autocorrect import Speller"
      ],
      "metadata": {
        "id": "-8Bo1gH__YCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Também tem em portugues!\n",
        "\n",
        "spell = Speller(lang='pt')\n",
        "spell('asúcar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MoQ3kf6SrWsi",
        "outputId": "fa3a312b-3797-470c-9cce-6479d9da0c3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'açúcar'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spell = Speller(lang='en')\n",
        "spell('Natureal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7J1rCozerohV",
        "outputId": "486f427f-397d-47c8-f87b-2d86159423a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Natural'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = word_tokenize('Ntural Luanguage Processin deals with'\\\n",
        "                         'the art of extrctinh insightes from'\\\n",
        "                         'Natual Languaes')"
      ],
      "metadata": {
        "id": "Td0eP5E0ruv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy8q6EYJsOYR",
        "outputId": "a7876232-7f9b-4ef9-c0ea-2885ec1f97ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Ntural', 'Luanguage', 'Processin', 'deals', 'withthe', 'art', 'of', 'extrctinh', 'insightes', 'fromNatual', 'Languaes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_spelling(tokens):\n",
        "  sentence_corrected = ' '.join([spell(word)\\\n",
        "                                 for word in tokens])\n",
        "  return sentence_corrected"
      ],
      "metadata": {
        "id": "Jvmc0r3IsSo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct_spelling(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "274X67pHsf9Y",
        "outputId": "82a42b82-5159-4c8d-aee6-118a55abf6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural Language Processing deals withthe art of extracting insights fromNatual Languages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming <br>\n",
        "Exercise 1.07: Using Stemming"
      ],
      "metadata": {
        "id": "2X8ns_Pc_YiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/72058182/227818410-728b3eca-c849-46b4-ac04-aeeab98a1662.png\">"
      ],
      "metadata": {
        "id": "dd1cvDCSqJZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import stem"
      ],
      "metadata": {
        "id": "ZltGBX06_eY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stems(word, stemmer):\n",
        "  return stemmer.stem(word)\n",
        "\n",
        "porterStem = stem.PorterStemmer()"
      ],
      "metadata": {
        "id": "8sKFWAODqRfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_stems('production', porterStem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Y5zTLLILqRbN",
        "outputId": "3879d342-8729-4c61-9569-a9fb1d385a28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'product'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_stems('coming', porterStem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uTZv4OK5qujS",
        "outputId": "c52280ee-13b5-4813-e932-78e258fcfb6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'come'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_stems('firing', porterStem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UFPzWPoVqv3V",
        "outputId": "d616b02e-a06f-477b-dddf-5354200eb805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fire'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# O stemmer também tem a língua portuguesa disponível\n",
        "\n",
        "stemmer = stem.SnowballStemmer('portuguese')\n",
        "get_stems('andando', stemmer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oIkWzZ3kq3ot",
        "outputId": "af3e6bb9-1733-4585-afc5-493062f4b7e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'andand'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_stems('corrigindo', stemmer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cPJziOefrB3K",
        "outputId": "d4be6950-7ce0-4d03-848f-1926261da3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'corrig'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando em inglês como no exemplo\n",
        "\n",
        "stemmer = stem.SnowballStemmer('english')\n",
        "get_stems('battling', stemmer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BHH4OQr2rS9I",
        "outputId": "0527d2cc-4735-4704-a0d4-2c0c6127a07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'battl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization <br>\n",
        "Exercise 1.08: Extracting the base Word using Lemmatization"
      ],
      "metadata": {
        "id": "GWGq4WnR_e0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algumas vezes o processo de stemização ou setamtização pode levar a alguns erros, como no exemplo, o resultado ser uma palavra que não existe como `battl`. Para isso existe a técnica de lematização, que é o processo de converter as palavras em sua forma gramatical básica. O exemplo citado da palavra `battling` seria processada como `battle`."
      ],
      "metadata": {
        "id": "I3nohNBJrfzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download\n",
        "download('wordnet')\n",
        "\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "2K8pdcU0_qA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a18632-390d-4ff4-a45a-a7c061732b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "lhVTreSQtWa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lemma(word):\n",
        "  return lemmatizer.lemmatize(word)\n",
        "\n",
        "get_lemma('products')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YZVD8emqtZ2w",
        "outputId": "a6ad0749-4cf5-4dc7-97b8-d757061afcbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'product'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_lemma('production')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KE3i1C4qtj9W",
        "outputId": "7ca4679e-2fb7-481c-8d37-2637a0480ae6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'production'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named Entity Recognition NER<br>\n",
        "Exercise 1.09: Treating Named Entities"
      ],
      "metadata": {
        "id": "VZZNHCYU_sM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import download \n",
        "from nltk import pos_tag \n",
        "from nltk import ne_chunk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "\n",
        "download('maxent_ne_chunker')\n",
        "download('words')\n",
        "\n",
        "nltk.download(['punkt', 'averaged_perceptron_tagger'])"
      ],
      "metadata": {
        "id": "wtDkem51_65F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da25e09c-76cd-4fa2-8e68-64e255ca4cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'We are reading a book published by Packt'\\\n",
        "           'wich is based out in Birmingham'"
      ],
      "metadata": {
        "id": "M_OAxh0VuA2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ner(text):\n",
        "  i = ne_chunk(pos_tag(word_tokenize(text)), binary=True)\n",
        "  return [a for a in i if len(a)==1]"
      ],
      "metadata": {
        "id": "3byt6J_KuNpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ner(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og4qEkNUuk8d",
        "outputId": "ad05cffd-ceb6-4738-b9f2-008ca106abe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tree('NE', [('Packtwich', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Sense Disambiguation <br>\n",
        "Exercise 1.10: Word Sense Disabiguation"
      ],
      "metadata": {
        "id": "Q4jEVOQm_-Ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/72058182/227821273-1a3be90f-9af3-4e85-9d7c-41e230f1b6fe.png\">"
      ],
      "metadata": {
        "id": "KtfWP3bbvWQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Palavras iguais em contextos diferentes podem ter significados diferentes e essa é uma das principais características da <b>ambiguidade</b>. <br>\n",
        "Traduzido literalmente como <i>'desambiguação de sentido de palavra'</i>, esse processo consiste em mapear uma palavra de acordo com o sentido que ela deveria carregar. <br>\n",
        "Nesse caso cada significado ambíguio é salvo em um synset de fundo (background synset).\n",
        "1. Play: participar de um esporte ou jogo\n",
        "2. Play: utilizar um instrumento musical\n",
        "\n",
        "Aí encontraremos a similaridade entre o contexto da palavra `play` no texto e cada uma das definições precedentes. A definição que se sair melhor, ou que for mais similar, 'ganha' a definição da palavra."
      ],
      "metadata": {
        "id": "JeNwLZFRvhzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.wsd import lesk\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "iYv5UT59AHv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e15a9b1-cc5e-4100-8298-ac2f2f685295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_1 = 'Keep your savings in the bank'\n",
        "sentence_2 = 'Its so risk to drive over the banks of the road'"
      ],
      "metadata": {
        "id": "jEg50PrPzSQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synset(sentence, word):\n",
        "  return lesk(word_tokenize(sentence), word)"
      ],
      "metadata": {
        "id": "6Wnbf6x8zkQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checando os sentidos da palavra 'bank' passando a sentença como parâmetro de avaliação."
      ],
      "metadata": {
        "id": "2QDBF3Qb0af2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_synset(sentence_1, 'bank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO5XtRIFz9P1",
        "outputId": "1a151e78-10bf-4884-e349-c424ab6035e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('savings_bank.n.02')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_synset(sentence_2, 'bank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PiT1YKYz2h-",
        "outputId": "7ad76897-f949-4393-a87c-413c6a086712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Synset('bank.v.07')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após verificar, podemos observar que o lesk consegue identificar que existem contextos diferentes nessas duas sentenças - n.02 e v.07"
      ],
      "metadata": {
        "id": "w1iWHr7w0h3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Boundary Detection <br>\n",
        "Exercise 1.11: Sentence boundary detection"
      ],
      "metadata": {
        "id": "QMpoNohWAKAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence boundary detection is the method of detecting where one sentence ends and another begins. "
      ],
      "metadata": {
        "id": "3tdxUSSh1jmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "nH72kFI4ATD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences(text):\n",
        "  return sent_tokenize(text)\n",
        "\n",
        "  \n",
        "get_sentences('We are reading a book. Do you know who is '\\\n",
        "              'the publisher? It is Packt. Packt is based'\\\n",
        "              'out of Birmingham')  "
      ],
      "metadata": {
        "id": "3WJfHAKJ12P9",
        "outputId": "95d0c8ca-5323-427d-ea0a-592dfb82f32a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We are reading a book.',\n",
              " 'Do you know who is the publisher?',\n",
              " 'It is Packt.',\n",
              " 'Packt is basedout of Birmingham']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_sentences('Mr Donald John Trump is the curent'\\\n",
        "              'president of the USA. Before joining'\\\n",
        "              'politics, he was a businessman')"
      ],
      "metadata": {
        "id": "6PZlUO2p2QPn",
        "outputId": "f3494bc0-8c13-4582-83dd-52317ff5081f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mr Donald John Trump is the curentpresident of the USA.',\n",
              " 'Before joiningpolitics, he was a businessman']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activity 1.01: Preprocessing of raw text"
      ],
      "metadata": {
        "id": "hRGk6jYbATkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow these steps to implement this activity: <br>\n",
        "\n",
        "1. Import the necessary libraries;\n",
        "2. Load que text corpus to a variable;\n",
        "3. Apply the tokenization process to the text corpus and print the first 20 tokens;\n",
        "4. Apply spelling correction on each token and print the initial 20 corrected tokens as well as the corrected text corpus;\n",
        "5. Apply PoS Tags to each of the corrected tokens and print them;\n",
        "6. Remove stop words from the corrected token list and print the initial 20 tokens;\n",
        "7. Apply stemming and lemmatization to the corrected list and then print the 20 initial 20 tokens\n",
        "8. Detect sentence boundaries in the gigen text corpus and print the total number os sentences"
      ],
      "metadata": {
        "id": "9z6EXMGg4Q8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import the necessary libraries\n",
        "\n",
        "import nltk"
      ],
      "metadata": {
        "id": "t-7byEQjAasQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load que text corpus to a variable\n",
        "\n",
        "infile = open('file.txt', 'r')\n",
        "data = infile.read()\n",
        "data"
      ],
      "metadata": {
        "id": "Wo192ybQeTxr",
        "outputId": "a06f9f3b-e61c-4b9d-e4d7-7944bd81028a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The reader of this course should have a basic knowledge of the Python programming lenguage.\\nHe/she must have knowldge of data types in Python.He should be able to write functions,\\n and also have the ability to import and use libraries and packages in Python. Familiarity\\n with basic linguistics and probability is assumed although not required to fully\\n complete this course.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Apply the tokenization process to the text corpus and print the first 20 tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "VOVSEe9OfFTY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Apply spelling correction on each token and print the initial 20 corrected tokens as well as the corrected text corpus\n"
      ],
      "metadata": {
        "id": "H7Tq-luufWgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Apply PoS Tags to each of the corrected tokens and print them\n"
      ],
      "metadata": {
        "id": "R4lHwLIbfeW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. "
      ],
      "metadata": {
        "id": "vMxmlIdJfjn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>Kick Starting a NLP Project</b></h3><br>\n",
        "\n",
        "We can divide a NLP project into several sub-projects or phases. These phases are completed in particular sequence. This tends to increase the overall efficiency of the process, as memory usage changes from one phase to the next. An NLP project has to go through six major phases, wich are outlined in the following figure:"
      ],
      "metadata": {
        "id": "lqplzoT9Icon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/72058182/227945836-e48ce886-57b3-4af8-91ee-6c7897ccab9c.png\">"
      ],
      "metadata": {
        "id": "xWcT8bkfJYNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose you are working on a project in wich you need to classify e-mails as important and unimportant. We will explain how this is carried out by discussing each phase in detail."
      ],
      "metadata": {
        "id": "TBwljf2XJb2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>Data Collection</b></h3>\n",
        "This is the initial phase of any NLP project. Our sole purpose is to collect data as per our requirements. For this, we may either use existing data, collect data from various online repositories, or create our own dataset by crawling the web. In our case, we will collect different email data. We can even get this data from our personal emails as well, to start with."
      ],
      "metadata": {
        "id": "4Dp_lTXjJ4mS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>Data Preprocessing</b></h3>\n",
        "Once the data is collected, we need to clean it. For the process of cleaning, we will make use of the difference preprocesssing steps that we have learned about in this chapter. It is necessary to clean the collected data to ensure effectiveness and accuracy. In our case,  we will follow these preprocessing steps:\n",
        "\n",
        "1. <i>Converting all the text data to lowercase</i>\n",
        "2. <i>Stop word removal</i>\n",
        "3. <i>Text normalization, wich will include replacing all numbers with some common term replacing punctuation with empty strings</i>\n",
        "4. <i>Stemming and lemmatization</i>"
      ],
      "metadata": {
        "id": "p5_CXZVhLO2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>Feature Extraction</b></h3>\n",
        "Computers understand only binary digits: 0 and 1. As such, every instruction we feed into a computer gets transformed into binary digits. similarly, machine learning models tend to understand only numeric data. Therefore, it becomes necessary to convert text data into its equivalent numerical form.\n",
        "\n",
        "To convert every email into its equivalent form, we will create a dictionary of all the unique words in our data and assing a unique index to each words. Then, we will represent every email with a list having a length equal to the number of unique words in the data. Ths list will have 1 at the indices of words that are present in the email and 0 ate the other indices. This is called one-hot-encoding. We will learn more about this in coming chapters."
      ],
      "metadata": {
        "id": "cwqp-tTWLVZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>Model Development</b></h3>\n",
        "Once the features set is ready, we need to develop a suitable model that can be trained to gain knowledge from the data. These models are generally statistical, machine learning-based, deep learning-based, or reinforcement learning-based. In our case, we will build a model that is capable of differentiating between important and inumportant emails."
      ],
      "metadata": {
        "id": "-KYXnS_vLZvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>Model Assessment</b></h3>\n",
        "After developing a model, it is essential to benchmark it. This process of benchmarking is known as model assessment. In this step, we will evaluate the performance of our model by comparing it to others. This can be done by using different parameters or metrics. These parameters include precision, recall, and accuracy. In our case, we will evaluate the newly created model by seeing how well it performs ate classifying emails as import and unimportant."
      ],
      "metadata": {
        "id": "zmv-Qk95LmUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3><b>Model Deployment</b></h3>\n",
        "This is the final stage and for most industrial NLP projects. In this stage, the models are put into production. They are either integrated into an existing system or new products are created by keeping this model as a base. In our case, we will deploy our model to production, so that can classify emails as important as unimportant in real time."
      ],
      "metadata": {
        "id": "-rrlxsoELqFJ"
      }
    }
  ]
}